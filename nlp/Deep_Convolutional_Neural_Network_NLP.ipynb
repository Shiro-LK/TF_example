{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Convolutional Neural Network NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SWiZIKl-6rZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.layers import dense, conv1d,  conv2d, dropout, max_pooling2d, max_pooling1d, batch_normalization\n",
        "from tensorflow.contrib.layers import flatten\n",
        "from tensorflow.nn import relu, relu6\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ka8N2JXR7kiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Implementation of VDCNN Architecture\n",
        "# Very Deep Convolution Neural Network for text classification\n",
        "# use word embedding instead of char embedding\n",
        "# link of paper : \n",
        "def Conv_Block(inputs, filters, kernel_size, name, stride=1, optional_shortcut = True):\n",
        "  \"\"\"\n",
        "    Convolution block, each convolution is followed by batch norm and relu function\n",
        "    @optional_shortcut : parameters which activate or not the residual\n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  conv_1 = conv1d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=stride, padding=\"same\", name= name + \"_conv_1\")\n",
        "  batch_norm_1 = batch_normalization(conv_1, name=name + \"_batchnorm_1\")\n",
        "  relu_1 = relu(batch_norm_1, name=name + \"relu_1\")\n",
        "  \n",
        "  conv_2 = conv1d(inputs=relu_1, filters=filters, kernel_size=kernel_size, strides=stride, padding=\"same\", name= name + \"_conv_2\")\n",
        "  batch_norm_2 = batch_normalization(conv_2, name=name + \"_batchnorm_2\")\n",
        "  relu_2 = relu(batch_norm_2, name=name + \"relu_2\")\n",
        "  \n",
        "  if optional_shortcut:\n",
        "    shorcut = conv1d(inputs=inputs, filters=filters, kernel_size=1, strides=stride, padding=\"same\", name= name + \"_shorcut\")\n",
        "    batch_norm_shorcut = batch_normalization(shorcut, name=name + \"_shorcut_batchnorm_2\")\n",
        "    relu_shortcut = relu(batch_norm_shorcut, name=name + \"_shorcut_relu_2\")\n",
        "    return relu_shortcut + relu_2\n",
        "  \n",
        "  else:\n",
        "    return relu_2\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "def VDCNN(inputs, vocabulary_size, embedding_size=16, kmax=8, size_sentences=100, classes=2):\n",
        "  \"\"\"\n",
        "    @inputs : array of sentences (each word is represented by a number)\n",
        "    @vocabulary_size : size of the vocabulary\n",
        "    @embedding_size : a small size is adviced when we use char embedding. If we use word embedding, bigger dimension are required\n",
        "  \"\"\"\n",
        "  # create embedding\n",
        "  #init_embeddings = tf.random_uniform(, -1.0, 1.0)\n",
        "  word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, embedding_size])\n",
        "  embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, inputs, name=\"lookupTable\")\n",
        "  #n = size_sentences\n",
        "  # conv layers\n",
        "  \n",
        "  x = conv1d(inputs=embedded_word_ids, filters=64, kernel_size=3, strides=1, padding=\"same\", name= \"conv_0\")\n",
        "  with tf.name_scope(\"ConvBlock\"):\n",
        "    nblocks = 3\n",
        "    for i in range(0, nblocks):\n",
        "      x = Conv_Block(x, filters=64*(2**i), kernel_size=3, name=\"ConvBlock1_\"+str(i))\n",
        "      x = Conv_Block(x, filters=64*(2**i), kernel_size=3, name=\"ConvBlock2_\"+str(i))\n",
        "      x = max_pooling1d(x, pool_size=3, strides=2, name = \"Maxpool_\" + str(i))\n",
        "     # n = n/2\n",
        "    i = i + 1\n",
        "\n",
        "    x = Conv_Block(x, filters=64*(2**i), kernel_size=3, name=\"ConvBlock1_\"+str(i))\n",
        "    x = Conv_Block(x, filters=64*(2**i), kernel_size=3, name=\"ConvBlock2_\"+str(i))\n",
        "\n",
        "    #n = int(n//kmax)\n",
        "  \n",
        "  # === K Max Pooling === #\n",
        "  with tf.name_scope(\"k-max-pooling\"):\n",
        "            x = tf.transpose(x, [0, 2, 1])\n",
        "            x = tf.nn.top_k(x, k=kmax, sorted=False).values\n",
        "\n",
        "  \n",
        "  with tf.name_scope(\"Full-Connected-Layers\"):\n",
        "    \n",
        "    x = flatten(x)\n",
        "\n",
        "    # === FC Layers === #\n",
        "    x = dropout(x, 0.5)\n",
        "    x = dense(x, 2048, name=\"dense_1\", activation=\"relu\")\n",
        "    x = dropout(x, 0.5)\n",
        "    x = dense(x, 2048, name=\"dense_2\", activation=\"relu\")\n",
        "    x = dropout(x, 0.5)\n",
        "    x = dense(x, classes, name=\"dense_output\", activation=None)\n",
        "\n",
        "  return x\n",
        "\n",
        "def VDCNN_light(inputs, vocabulary_size, embedding_size=16, kmax=8, size_sentences=100, classes=2):\n",
        "  \"\"\"\n",
        "    @inputs : array of sentences (each word is represented by a number)\n",
        "    @vocabulary_size : size of the vocabulary\n",
        "    @embedding_size : a small size is adviced when we use char embedding. If we use word embedding, bigger dimension are required\n",
        "  \"\"\"\n",
        "  # create embedding\n",
        "  #init_embeddings = tf.random_uniform(, -1.0, 1.0)\n",
        "  word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, embedding_size])\n",
        "  embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, inputs, name=\"lookupTable\")\n",
        "\n",
        "  # conv layers\n",
        "  \n",
        "  x = conv1d(inputs=embedded_word_ids, filters=64, kernel_size=3, strides=1, padding=\"same\", name= \"conv_0\")\n",
        "  \n",
        "  with tf.name_scope(\"ConvBlock\"):\n",
        "    nblocks = 3\n",
        "    for i in range(0, nblocks):\n",
        "      x = Conv_Block(x, filters=64*(2**i), kernel_size=3, name=\"ConvBlock1_\"+str(i))\n",
        "      x = max_pooling1d(x, pool_size=3, strides=2, name = \"Maxpool_\" + str(i))\n",
        "    i = i + 1\n",
        "\n",
        "    x = Conv_Block(x, filters=64*(2**i), kernel_size=3, name=\"ConvBlock1_\"+str(i))\n",
        "  \n",
        "  # === K Max Pooling === #\n",
        "  with tf.name_scope(\"k-max-pooling\"):\n",
        "            x = tf.transpose(x, [0, 2, 1])\n",
        "            x = tf.nn.top_k(x, k=kmax, sorted=False).values\n",
        "\n",
        "  \n",
        "  with tf.name_scope(\"Full-Connected-Layers\"):\n",
        "    \n",
        "    x = flatten(x)\n",
        "\n",
        "    # === FC Layers === #\n",
        "    x = dropout(x, 0.5)\n",
        "    x = dense(x, 512, name=\"dense_1\", activation=\"relu\")\n",
        "    #x = dropout(x, 0.5)\n",
        "    #x = dense(x, 512, name=\"dense_2\", activation=\"relu\")\n",
        "    x = dropout(x, 0.5)\n",
        "    x = dense(x, classes, name=\"dense_output\", activation=None)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dA0F5-vn7krQ",
        "colab_type": "code",
        "outputId": "f829f103-42be-4ec6-d27b-10de09f67b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "imdb = keras.datasets.imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "train_labels= train_labels.reshape(-1,1)\n",
        "test_labels = test_labels.reshape(-1,1)\n",
        "max_=0\n",
        "min_=10000\n",
        "for s in train_data:\n",
        "  max_ = max(max_, len(s))\n",
        "  min_ = min(min_, len(s))\n",
        "for s in test_data:\n",
        "  max_ = max(max_, len(s))\n",
        "  min_ = min(min_, len(s))\n",
        "print(\"Size min/max sentences : {0}/{1}\".format(min_, max_))\n",
        "print(\"train : {0} {1}\".format(len(train_data), train_labels.shape))\n",
        "print(\"test : {0} {1}\".format(len(test_data), test_labels.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Size min/max sentences : 7/2494\n",
            "train : 25000 (25000, 1)\n",
            "test : 25000 (25000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5FiTlU0U7kyY",
        "colab_type": "code",
        "outputId": "8ed03872-ba07-49ac-c7ab-2f04ab5afaaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## preprocessing on data, neural network need same dimension inputs for batch\n",
        "def decode_review(text, dic):\n",
        "    return ' '.join([dic[i] for i in text])\n",
        "\n",
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decode_review(train_data[0], reverse_word_index)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "PFPUfBzf9fhr",
        "colab_type": "code",
        "outputId": "98dbf9bf-e00f-4784-d6ed-07496ae30b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_labels[0,:])\n",
        "print(type(train_data[0]))\n",
        "print(train_data.shape, train_labels.shape)\n",
        "print(train_data[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n",
            "<class 'list'>\n",
            "(25000,) (25000, 1)\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zIb0ZQ6vlKil",
        "colab_type": "code",
        "outputId": "b1432918-c06d-4fef-d301-35093b90afed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# preprocessing, add padding\n",
        "max_=128 #size max of a sentence\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=max_)\n",
        "train_data = train_data.astype(np.int32)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=max_)\n",
        "test_data = test_data.astype(np.int32)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "print(train_labels.shape, test_labels.shape)\n",
        "print(train_labels[0], test_labels[0])\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "print(train_labels.shape, test_labels.shape)\n",
        "print(train_labels[0], test_labels[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 1) (25000, 1)\n",
            "[1] [0]\n",
            "(25000, 2) (25000, 2)\n",
            "[0. 1.] [1. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDpOSEhPgJ4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjI0EDg6q67G",
        "colab_type": "code",
        "outputId": "07f5757d-e58a-4643-806b-1e8c9741af7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "cell_type": "code",
      "source": [
        "# tf graph\n",
        "classes = 2\n",
        "tf.reset_default_graph()\n",
        "# 0 negatif, 1 positif\n",
        "x =  tf.placeholder(tf.int32, [None, max_], name=\"x\")\n",
        "y = tf.placeholder(tf.int32, [None, classes], name=\"y\")\n",
        "model = VDCNN(inputs=x, vocabulary_size=10000, embedding_size=300, kmax=8, size_sentences= max_, classes=classes)\n",
        "\n",
        "# ====== Loop Training ===== #\n",
        "batch_size = 128\n",
        "batch_size_test = 256\n",
        "total_training = len(train_data)\n",
        "total_test = len(test_data)\n",
        "nb_epochs = 10\n",
        "\n",
        "preds = tf.nn.softmax(model)\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=y))\n",
        "accuracy = tf.reduce_mean( tf.cast(tf.equal(tf.argmax(preds, 1, output_type=tf.int32), tf.argmax(y, 1, output_type=tf.int32)), tf.float32))\n",
        "train_step = tf.contrib.opt.NadamOptimizer(0.0001).minimize(loss)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  numberIteration = total_training//batch_size\n",
        "  numberIteration_test = int(np.ceil(total_test/batch_size_test))\n",
        "  print(\"Number of Iteration per epoch : {}\".format(numberIteration))\n",
        "  print(\"Number of Iteration per epoch test : {}\".format(numberIteration_test))\n",
        "  for e in range(0, nb_epochs):\n",
        "    \n",
        "    # acc and loss for each batch\n",
        "    acc_ = np.zeros((numberIteration,))\n",
        "    loss_ = np.zeros((numberIteration,))\n",
        "    \n",
        "    begin = time.time()\n",
        "    for i in range(0, numberIteration):\n",
        "\n",
        "      \n",
        "      choice = np.random.randint(0, total_training, batch_size, dtype=np.int32)\n",
        "      \n",
        "      train_batch = {x : train_data[choice,:] , y : train_labels[choice,:]}\n",
        "      \n",
        "      _, accuracy_iteration, loss_iteration, preds_iteration = sess.run([train_step, accuracy, loss, preds], feed_dict=train_batch)\n",
        "      \n",
        "      acc_[i] = accuracy_iteration\n",
        "      loss_[i] = loss_iteration\n",
        "      #print(\"# === Training Iteration n°{0} === # \\n loss CE : {1} -- accuracy : {2}\". format(i, np.mean(loss_[:i+1]), np.mean(acc_[:i+1])))\n",
        "      #print(preds_iteration)\n",
        "      #cal = tf.reduce_mean(-tf.reduce_sum(train_batch[y] * tf.log(preds_iteration), 1))\n",
        "      #print(cal.eval())\n",
        "      #cal2= np.equal(np.argmax(preds_iteration, axis=1), np.argmax(train_batch[y], axis=1))\n",
        "      #print(np.sum(cal2), np.sum(cal2)/batch_size)\n",
        "      #print(loss_iteration, accuracy_iteration)\n",
        "    print(\"# === Training epoch n°{0}  {1} sec === # \\n loss CE : {2} -- accuracy : {3}\". format(e, time.time()-begin, np.mean(loss_), np.mean(acc_)))\n",
        "    \n",
        "    # == Testing step == #\n",
        "    accuracy_test = np.zeros((numberIteration_test,))\n",
        "    loss_test = np.zeros((numberIteration_test,))\n",
        "    begin = time.time()\n",
        "    \n",
        "    for i in range(0, numberIteration_test):\n",
        "      if i == numberIteration_test -1:\n",
        "        test_batch ={x : test_data[i*batch_size_test:, :], y : test_labels[i*batch_size_test:, :] }\n",
        "        accuracy_iteration_test, loss_iteration_test = sess.run([accuracy, loss], feed_dict=test_batch)\n",
        "        \n",
        "        accuracy_test[i] = accuracy_iteration_test * (total_test-i*batch_size_test)\n",
        "        loss_test[i] = loss_iteration_test * (total_test-i*batch_size_test)\n",
        "        \n",
        "      else:\n",
        "        test_batch ={x : test_data[i*batch_size_test:(i+1)*batch_size_test, :], y : test_labels[i*batch_size_test:(i+1)*batch_size_test, :] }\n",
        "        accuracy_iteration_test, loss_iteration_test = sess.run([accuracy, loss], feed_dict=test_batch)\n",
        "        \n",
        "        accuracy_test[i] = accuracy_iteration_test * batch_size_test\n",
        "        loss_test[i] = loss_iteration_test * batch_size_test\n",
        "      #print(\"# === Testing Iteration n°{0} === # \\n loss CE : {1} -- accuracy : {2}\". format(i, np.sum(loss_test[:i+1])/((i+1)*batch_size_test), np.sum(accuracy_test[:i+1])/((i+1)*batch_size_test)))\n",
        "\n",
        "    print(\"# === Testing epoch n°{0}  {1} sec === # \\n loss CE : {2} -- accuracy : {3}\". format(e, time.time()-begin, np.sum(loss_test)/total_test, np.sum(accuracy_test)/total_test))\n",
        "    #print(loss_test)\n",
        "  \n",
        "#278\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Iteration per epoch : 195\n",
            "Number of Iteration per epoch test : 98\n",
            "# === Training epoch n°0  36.298972845077515 sec === # \n",
            " loss CE : 0.6705681672463051 -- accuracy : 0.5405048076923077\n",
            "# === Testing epoch n°0  18.152676820755005 sec === # \n",
            " loss CE : 0.5358535016345978 -- accuracy : 0.728800000076294\n",
            "# === Training epoch n°1  35.23712611198425 sec === # \n",
            " loss CE : 0.30142282572312235 -- accuracy : 0.875\n",
            "# === Testing epoch n°1  17.86713171005249 sec === # \n",
            " loss CE : 0.3642820947170258 -- accuracy : 0.847560000076294\n",
            "# === Training epoch n°2  35.085737466812134 sec === # \n",
            " loss CE : 0.19372028811619832 -- accuracy : 0.9290064102564103\n",
            "# === Testing epoch n°2  17.871349573135376 sec === # \n",
            " loss CE : 0.3937003105306625 -- accuracy : 0.8577200000762939\n",
            "# === Training epoch n°3  35.1430127620697 sec === # \n",
            " loss CE : 0.12421411780210642 -- accuracy : 0.9587339743589743\n",
            "# === Testing epoch n°3  17.851994037628174 sec === # \n",
            " loss CE : 0.4242630764722824 -- accuracy : 0.853920000076294\n",
            "# === Training epoch n°4  35.2651424407959 sec === # \n",
            " loss CE : 0.11647630623326852 -- accuracy : 0.9685096153846153\n",
            "# === Testing epoch n°4  17.849092483520508 sec === # \n",
            " loss CE : 0.33805147364616395 -- accuracy : 0.8627200001335144\n",
            "# === Training epoch n°5  35.12822771072388 sec === # \n",
            " loss CE : 0.05248172469675923 -- accuracy : 0.9856971153846154\n",
            "# === Testing epoch n°5  17.82679796218872 sec === # \n",
            " loss CE : 0.47133416588783267 -- accuracy : 0.862959999885559\n",
            "# === Training epoch n°6  35.18195343017578 sec === # \n",
            " loss CE : 0.06476657831539902 -- accuracy : 0.9872596153846154\n",
            "# === Testing epoch n°6  17.817979335784912 sec === # \n",
            " loss CE : 0.5912196973133087 -- accuracy : 0.8608800001335144\n",
            "# === Training epoch n°7  35.12198185920715 sec === # \n",
            " loss CE : 0.017511593737006666 -- accuracy : 0.9959134615384615\n",
            "# === Testing epoch n°7  17.847841262817383 sec === # \n",
            " loss CE : 0.6072435945320129 -- accuracy : 0.8607200001335144\n",
            "# === Training epoch n°8  35.01327109336853 sec === # \n",
            " loss CE : 0.009383978090810183 -- accuracy : 0.9973157051282051\n",
            "# === Testing epoch n°8  17.806828022003174 sec === # \n",
            " loss CE : 0.8314449830818176 -- accuracy : 0.8446400000572205\n",
            "# === Training epoch n°9  35.1226863861084 sec === # \n",
            " loss CE : 0.04367305813491559 -- accuracy : 0.9935897435897436\n",
            "# === Testing epoch n°9  17.809399843215942 sec === # \n",
            " loss CE : 0.856560813293457 -- accuracy : 0.8543600000953674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "phK8rAqG8dz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}