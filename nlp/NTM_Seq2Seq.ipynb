{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NTM-Seq2Seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "F-lyzE4POzYz",
        "colab_type": "code",
        "outputId": "eeb87215-bb6b-499c-95f4-b34897b49754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "import random\n",
        "from tensorflow.contrib.cudnn_rnn import CudnnLSTM\n",
        "from tensorflow.contrib.rnn import LSTMCell, GRUCell\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "IYmAKvhlcmpZ",
        "colab_type": "code",
        "outputId": "d10ddec4-45a4-47bd-c116-cfa5e9782f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "## --- Download Dataset --- #\n",
        "#!wget http://www.statmt.org/europarl/v7/fr-en.tgz\n",
        "# !tar zxvf fr-en.tgz\n",
        "#!wget http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz\n",
        "#!tar zxvf training-parallel-europarl-v7.tgzhttps://raw.githubusercontent.com/udacity/cn-deep-learning/master/language-translation/data/small_vocab_fr\n",
        "!wget https://raw.githubusercontent.com/udacity/cn-deep-learning/master/language-translation/data/small_vocab_fr\n",
        "!wget https://raw.githubusercontent.com/udacity/cn-deep-learning/master/language-translation/data/small_vocab_en\n",
        "!ls -l"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-27 16:51:34--  https://raw.githubusercontent.com/udacity/cn-deep-learning/master/language-translation/data/small_vocab_fr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10135742 (9.7M) [text/plain]\n",
            "Saving to: ‘small_vocab_fr’\n",
            "\n",
            "\rsmall_vocab_fr        0%[                    ]       0  --.-KB/s               \rsmall_vocab_fr      100%[===================>]   9.67M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-02-27 16:51:35 (114 MB/s) - ‘small_vocab_fr’ saved [10135742/10135742]\n",
            "\n",
            "--2019-02-27 16:51:36--  https://raw.githubusercontent.com/udacity/cn-deep-learning/master/language-translation/data/small_vocab_en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9085267 (8.7M) [text/plain]\n",
            "Saving to: ‘small_vocab_en’\n",
            "\n",
            "small_vocab_en      100%[===================>]   8.66M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-02-27 16:51:37 (94.4 MB/s) - ‘small_vocab_en’ saved [9085267/9085267]\n",
            "\n",
            "total 18792\n",
            "drwx------ 3 root root     4096 Feb 27 13:15 drive\n",
            "drwxr-xr-x 1 root root     4096 Feb 19 17:17 sample_data\n",
            "-rw-r--r-- 1 root root  9085267 Feb 27 16:51 small_vocab_en\n",
            "-rw-r--r-- 1 root root 10135742 Feb 27 16:51 small_vocab_fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NTtOgl1scs2C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## --- Clean Dataset --- #\n",
        "\n",
        "\n",
        "####\n",
        "def load_data(filename, number_line=None, threshold=0):\n",
        "  \n",
        "  # open the file as read only\n",
        "  file = open(filename, mode='r', encoding='utf-8')\n",
        "  \n",
        "  # split the document into sentences\n",
        "  doc = []\n",
        "  count = Counter()\n",
        "  for i, line in enumerate(file):\n",
        "    if number_line is not None and i == number_line :\n",
        "      break\n",
        "    # normalize unicode characters\n",
        "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "    line = line.decode('UTF-8')\n",
        "    \n",
        "    # lower letter\n",
        "    line = line.lower().strip()\n",
        "    \n",
        "    # # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    line = re.sub(r\"([?.!,¿])\", r\" \\1 \", line)\n",
        "    line = re.sub(r'[\" \"]+', \" \", line)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    line = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", line)\n",
        "    \n",
        "    line = line.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    # line = '<start> ' + line + ' <end>'\n",
        "    \n",
        "    s = line.split()\n",
        "    count[len(s)] += 1\n",
        "    doc.append(s)  \n",
        "  \n",
        "  return doc, count\n",
        "  \n",
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang, threshold=0, target=True):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    if target:\n",
        "      self.CODES = ['<PAD>' , '<UNK>', '<GO>','<EOS>' ]\n",
        "      self.vocab.update(self.CODES)\n",
        "    else:\n",
        "      self.CODES = ['<PAD>' , '<UNK>']\n",
        "      self.vocab.update(self.CODES)\n",
        "    \n",
        "    if threshold > 0:\n",
        "      self.apply_threshold(threshold)\n",
        "    self.create_index()\n",
        "    \n",
        "    \n",
        "  # Transform word into UNK token if they appear few times only \n",
        "  def apply_threshold(self, threshold):\n",
        "    counter = Counter()\n",
        "    for sentence in self.lang:\n",
        "      for w in sentence:\n",
        "        counter[w] += 1\n",
        "        \n",
        "    for i, sentence in enumerate(self.lang):\n",
        "      for j, w in enumerate(sentence):\n",
        "        if counter[w] < threshold:\n",
        "          self.lang[i][j] = '<UNK>'\n",
        "      \n",
        "      \n",
        "      \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    #CODES = [('<PAD>', 0), ('<EOS>', 1), ('<UNK>', 2), ('<GO>', 3) ]\n",
        "    \n",
        "    \n",
        "    for i, w  in enumerate(self.CODES):\n",
        "      self.word2idx[w] = i\n",
        "      \n",
        "    index = len(self.CODES)\n",
        "    for i, word in enumerate(self.vocab):\n",
        "      if word not in self.CODES:\n",
        "        self.word2idx[word] = index\n",
        "        index += 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word\n",
        "      \n",
        "  def ConvertSentenceToIndex(self, s):\n",
        "    ans = []\n",
        "    for w in s:\n",
        "      if w in self.word2idx:\n",
        "        ans.append(self.word2idx[w])\n",
        "      else:\n",
        "        ans.append(self.word2idx['<UNK>'])\n",
        "        \n",
        "    return ans\n",
        "  \n",
        "  def ConvertIndexToSentence(self, s):\n",
        "    ans = []\n",
        "    for w in s:\n",
        "        ans.append(self.idx2word[w])\n",
        "\n",
        "        \n",
        "    return ans\n",
        "  \n",
        "  def ConvertTextToIndex(self, text, target=False, inplace=True):\n",
        "    \"\"\"\n",
        "      @input:\n",
        "      text : list of sentences. ex : [['I', 'am', 'John'], ['I', 'have', '10']]\n",
        "      target : if the text is the target : True. We add <GO> and <EOS> ; if the text is the source : False.\n",
        "      \n",
        "      @return:\n",
        "        the text converted in id text\n",
        "    \"\"\"\n",
        "    \n",
        "    if target:\n",
        "      \n",
        "      target_ids = []\n",
        "      \n",
        "      for sentence in text:\n",
        "        temp = []\n",
        "        # add <GO> indicator of the target sentence\n",
        "        #temp.append(self.word2idx['<GO>'])\n",
        "        # convert word of each sentence in ids\n",
        "        temp = temp + self.ConvertSentenceToIndex(sentence)\n",
        "        # add <EOS> indicator of the target sentence\n",
        "        temp.append(self.word2idx['<EOS>'])\n",
        "        \n",
        "        target_ids.append(temp)\n",
        "      \n",
        "      if inplace:\n",
        "        self.lang = target_ids\n",
        "      return target_ids\n",
        "    else:\n",
        "      source_ids = []\n",
        "      \n",
        "      for sentence in text:\n",
        "        source_ids.append(self.ConvertSentenceToIndex(sentence))\n",
        "      if inplace:\n",
        "        self.lang=source_ids\n",
        "      return source_ids\n",
        "    \n",
        "  def ConvertIndexToText(self, id_text):\n",
        "    \"\"\"\n",
        "      @input:\n",
        "      text : list of sentences. ex : [['I', 'am', 'John'], ['I', 'have', '10']]\n",
        "      target : if the text is the target : True. We add <GO> and <EOS> ; if the text is the source : False.\n",
        "      \n",
        "      @return:\n",
        "        the text converted in id text\n",
        "    \"\"\"\n",
        "    \n",
        "    text = []\n",
        "    for sentence in id_text:\n",
        "      text.append(self.ConvertIndexToSentence(sentence))\n",
        "\n",
        "    return text\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Bu8ij6F5zNm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FnZ2QlvNP_Cv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "### --- Network --- ###\n",
        "def init_placeholders():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
        "    source_sequence_length = tf.placeholder(tf.int32, [None], name='source_sequence_length')\n",
        "    \n",
        "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
        "    max_target_len = tf.reduce_max(target_sequence_length)\n",
        "\n",
        "    return inputs, targets, source_sequence_length, target_sequence_length, max_target_len\n",
        "  \n",
        "def hyperparam_inputs():\n",
        "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    return lr_rate, keep_prob\n",
        "    \n",
        "def build_encoder(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
        "                   source_vocab_size, encoding_embedding_size, source_sequence_length, use_lstm):\n",
        "    \"\"\"\n",
        "    :input:\n",
        "    @rnn_inputs : input data \n",
        "    @rnn_size : size of the rnn\n",
        "    @num_layers : number of rnn stacked\n",
        "    @kepp_proba : proba for dropout\n",
        "    @source_vocab_size : size of the vocabulary (used for embedding)\n",
        "    @encoding_embedding_size : size of the embedding\n",
        "    \n",
        "    :return: tuple (RNN output, RNN state)\n",
        "    \"\"\"\n",
        "    \n",
        "    # --- Embedding --- #\n",
        "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, vocab_size=source_vocab_size, embed_dim=encoding_embedding_size) #given index, return matrice \n",
        "                    \n",
        "    # --- RNN --- #\n",
        "    type_rnn = tf.contrib.rnn.LSTMCell if use_lstm else tf.contrib.rnn.GRUCell\n",
        "    \n",
        "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(type_rnn(rnn_size), keep_prob) for _ in range(num_layers)])\n",
        "    \n",
        "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
        "                                       embed, #sequence_length=source_sequence_length,\n",
        "                                       dtype=tf.float32) \n",
        "    # outputs represent the outputs of all timesteps, state the state/output of the last timestep\n",
        "    return outputs, state\n",
        "  \n",
        "def build_decoder(target_data, encoder_state,\n",
        "                   target_sequence_length, max_target_sequence_length, max_inference_sequence_length,\n",
        "                   rnn_size, num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, decoding_embedding_size, use_lstm):\n",
        "    \"\"\"\n",
        "    Create decoding layer\n",
        "    :input:\n",
        "    @target_data : target data \n",
        "    @encoder_state : feature map of the encoder\n",
        "    @target_sequence_length : tf.placeholder\n",
        "    @max_target_sequence_length : max value of target_sequence_length\n",
        "    @rnn_size : number of neurons for each rnn in the decoder\n",
        "    @num_layers : number of layers of RNN \n",
        "    @target_vocab_to_int : dic of the vocabulary to int\n",
        "    @target_vocab_size : size of the vocabulary target\n",
        "    @batch_size : size of the batch\n",
        "    @keep_prob : probability of the dropout\n",
        "    @decoding_embedding_size : size of the embedding target \n",
        "    \n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"              \n",
        "    # --- add <GO> to target data --- #\n",
        "    go_id = target_vocab_to_int['<GO>'] # adding GO token is necessarly for the trainingHelper\n",
        "\n",
        "    #after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), target_data], 1)\n",
        "\n",
        "    # --- Embedding Decoder --- #\n",
        "    target_vocab_size = len(target_vocab_to_int)\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, after_concat)\n",
        "\n",
        "    \n",
        "    type_rnn = tf.contrib.rnn.LSTMCell if use_lstm else tf.contrib.rnn.GRUCell\n",
        "    cells = tf.contrib.rnn.MultiRNNCell([type_rnn(rnn_size) for _ in range(num_layers)])\n",
        "    output_layer = tf.layers.Dense(target_vocab_size)\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        # --- Training decoding --- #\n",
        "        train_cell = tf.contrib.rnn.DropoutWrapper(cells, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "        # for only input layer\n",
        "        train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
        "                                                         target_sequence_length)\n",
        "\n",
        "        train_decoder = tf.contrib.seq2seq.BasicDecoder(train_cell, \n",
        "                                                  train_helper, \n",
        "                                                  encoder_state, \n",
        "                                                  output_layer)\n",
        "\n",
        "        # unrolling the decoder layer\n",
        "        train_output, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, \n",
        "                                                          impute_finished=True, \n",
        "                                                          maximum_iterations=max_target_sequence_length)\n",
        "\n",
        "\n",
        "        \n",
        "        # --- Inference decoding --- #\n",
        "        infer_cell = tf.contrib.rnn.DropoutWrapper(cells, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "        \n",
        "        start_of_sequence_id = target_vocab_to_int['<GO>']\n",
        "        end_of_sequence_id = target_vocab_to_int['<EOS>']\n",
        "        \n",
        "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
        "                                                          #start_of_sequence_id,\n",
        "                                                          tf.fill([batch_size], start_of_sequence_id), # add GO token for the beginning of inference\n",
        "                                                          end_of_sequence_id)\n",
        "\n",
        "        infer_decoder = tf.contrib.seq2seq.BasicDecoder(infer_cell, \n",
        "                                                  infer_helper, \n",
        "                                                  encoder_state, \n",
        "                                                  output_layer)\n",
        "\n",
        "        infer_output, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder, \n",
        "                                                          impute_finished=True, \n",
        "                                                          maximum_iterations=max_inference_sequence_length)# put sequence source lenght *2\n",
        "\n",
        "    return (train_output, infer_output)\n",
        "  \n",
        "def seq2seq_model(input_data, target_data, keep_prob, batch_size, target_vocab_to_int, source_sequence_length,\n",
        "                  target_sequence_length, max_target_sentence_length,\n",
        "                  source_vocab_size, target_vocab_size,\n",
        "                  enc_embedding_size=200, dec_embedding_size=200,\n",
        "                    rnn_size=128, num_layers=3, use_lstm=True):\n",
        "    \n",
        "    \"\"\"\n",
        "    Build the Sequence-to-Sequence model\n",
        "    :input:\n",
        "    @input_data  : tensor of size [batchsize, max lenght training sentences or None]\n",
        "    @target_data : tensor of size [batchsize, max lenght training  sentences target or None]\n",
        "    @keep_prob   : probability for dropout (tensor)\n",
        "    @batch_size  : size of the batch (tensor)\n",
        "    @target_sequence_length : size of the target sequence length\n",
        "    @max_target_sentence_lenght: \n",
        "    @source_vocab_size : size of the vocabulary of the input\n",
        "    @target_vocab_size : size of the vocabulary of the target (output) of NTM\n",
        "    @enc_embedding_size : size of the dimension of the encoder embedding\n",
        "    @dec_embedding_size : size of the dimension of the decoder embedding\n",
        "    @rnn_size : number of neurons for each rnn\n",
        "    @num_layers: number of layer of RNN in the encoder\n",
        "    @target_vocab_to_int : dictionnary with key as word, value as int\n",
        "    \n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_states = build_encoder(input_data, \n",
        "                                             rnn_size, \n",
        "                                             num_layers, \n",
        "                                             keep_prob, \n",
        "                                             source_vocab_size, \n",
        "                                             enc_embedding_size, source_sequence_length, use_lstm)\n",
        "    \n",
        "\n",
        "    max_inference_sequence_length = tf.reduce_max(source_sequence_length)*2\n",
        "    train_output, infer_output = build_decoder(target_data,\n",
        "                                               enc_states, \n",
        "                                               target_sequence_length, \n",
        "                                               max_target_sentence_length,\n",
        "                                               max_inference_sequence_length,\n",
        "                                               rnn_size,\n",
        "                                              num_layers,\n",
        "                                              target_vocab_to_int,\n",
        "                                              target_vocab_size,\n",
        "                                              batch_size,\n",
        "                                              keep_prob,\n",
        "                                              dec_embedding_size, use_lstm)\n",
        "    \n",
        "    return train_output, infer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUB8vy7ejZHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### --- Create batch function iterator --- ####\n",
        "def get_batch(sources, targets, source_vocab2int_pad, target_vocab2int_pad, batch_size=32, training=True):\n",
        "  \"\"\"\n",
        "    @input ::\n",
        "    \n",
        "    @source : source data (list of sentences)\n",
        "    @target : target data - translation of the source (list of sentences)\n",
        "    @source_vocab2int_pad : int value corresponding to the pad in the word2id for the source data\n",
        "    @target_vocab2int_pad : int value corresponding to the pad in the word2id for the target data\n",
        "  \"\"\"\n",
        "  \n",
        "  if len(sources) != len(targets):\n",
        "    raise(\"Check Source and Target data, not same dimension\")\n",
        "  \n",
        "  \n",
        "  if training:\n",
        "    n = len(sources)\n",
        "    for i in range(0, n//batch_size):\n",
        "      sample = np.random.randint(0, n, batch_size)\n",
        "      \n",
        "      source_batch = [sources[v] for v in sample]\n",
        "      target_batch = [targets[v] for v in sample]\n",
        "      \n",
        "      pad_source_batch = pad_sequences(source_batch, padding='post', value=source_vocab2int_pad)\n",
        "      pad_target_batch = pad_sequences(target_batch, padding='post', value=target_vocab2int_pad)\n",
        "      \n",
        "      ## compute the lenghts of the batch\n",
        "      pad_target_lengths = []\n",
        "      pad_source_lengths = []\n",
        "      \n",
        "      for i in range(batch_size):\n",
        "        pad_source_lengths.append( len(source_batch[i]) )\n",
        "        pad_target_lengths.append( len(target_batch[i]) )\n",
        "        \n",
        "      \n",
        "      yield pad_source_batch, pad_target_batch, pad_source_lengths, pad_target_lengths\n",
        "      \n",
        "  else:\n",
        "    for batch_i in range(0, len(sources)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:start_i + batch_size]\n",
        "        targets_batch = targets[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = pad_sequences(sources_batch, padding='post', value=source_vocab2int_pad)\n",
        "        pad_targets_batch = pad_sequences(targets_batch, padding='post', value=target_vocab2int_pad)\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
        "    \n",
        "    if start_i + batch_size < len(sources) - 1:\n",
        "        start_i += batch_size\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:]\n",
        "        targets_batch = targets[start_i:]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = pad_sequences(sources_batch, padding='post', value=source_vocab2int_pad)\n",
        "        pad_targets_batch = pad_sequences(targets_batch, padding='post', value=target_vocab2int_pad)\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
        "\n",
        "def get_accuracy(target, logits):\n",
        "    \"\"\"\n",
        "    Calculate accuracy\n",
        "    \"\"\"\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0,0),(0,max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0,0),(0,max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yoBWriwtH15q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "def find(liste, w):\n",
        "  try:\n",
        "    return liste.index(w)\n",
        "  except:\n",
        "    return -1\n",
        "\n",
        "def bleu_metric(y_true, y_pred, id_eos=\"<EOS>\", id_padding=\"<PAD>\"):\n",
        "  if len(y_true) != len(y_pred):\n",
        "    raise(\"Number of reference and hypothesis sentences differents, check them\")\n",
        "  else:\n",
        "    N = len(y_true)\n",
        "    bleu = np.zeros((N,))\n",
        "    for i in range(N):\n",
        "      y_true_limit = find(y_true[i], id_eos) if find(y_true[i], id_eos) != -1 else find(y_true[i], id_padding)\n",
        "      y_pred_limit = find(y_pred[i], id_eos) if find(y_pred[i], id_eos) != -1 else find(y_pred[i], id_padding)\n",
        "      ref = y_true[i][:y_true_limit] if y_true_limit !=-1 else y_true[i]\n",
        "      hyp = y_pred[i][:y_pred_limit] if y_pred_limit !=-1 else y_pred[i]\n",
        "      #print(ref, hyp)\n",
        "      try:\n",
        "        if len(ref)<=4 or len(hyp)<=4:\n",
        "          bleu[i] = sentence_bleu([ref], hyp, smoothing_function=SmoothingFunction().method4)\n",
        "        else:\n",
        "          bleu[i] = sentence_bleu([ref], hyp)\n",
        "      except:\n",
        "        pass#print(ref, hyp)\n",
        "      \n",
        "    return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l07P-pdDRfyd",
        "colab_type": "code",
        "outputId": "b4aee63e-4f30-4696-c8c6-2e658451bce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "ref = [['i', 'am', 'boy'], ['i', 'am', 'a', 'girl']]\n",
        "hyp = [['i', 'am',  'a','boy'], ['i', 'am', 'a', 'girl']]\n",
        "bleu_metric(ref, hyp, 1, 0)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'am', 'boy'] ['i', 'am', 'a', 'boy']\n",
            "['i', 'am', 'a', 'girl'] ['i', 'am', 'a', 'girl']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.28662276, 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "cdxpK0OPx_mF",
        "colab_type": "code",
        "outputId": "72804046-905b-4825-8b85-ef10ebfface5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# load dataset \n",
        "from sklearn.model_selection import train_test_split\n",
        "# --- load target --- #\n",
        "filename_fr = \"small_vocab_fr\"\n",
        "doc_fr, count_fr = load_data(filename_fr)\n",
        "\n",
        "print(\"## --- French --- ##\")\n",
        "print(\"number of sentences : {0}\\nmin: {1} -- max: {2}\".format(sum(count_fr.values()), min(count_fr.keys()), max(count_fr.keys())))\n",
        "\n",
        "# --- load english data --- #\n",
        "filename_en = 'small_vocab_en'\n",
        "doc_en, count_en = load_data(filename_en)\n",
        "\n",
        "print(\"## --- English --- ##\")\n",
        "print(\"number of sentences : {0}\\nmin: {1} -- max: {2}\".format(sum(count_en.values()), min(count_en.keys()), max(count_en.keys())))\n",
        "\n",
        "# --- create train/test data --- #\n",
        "source_train, source_test, target_train, target_test = train_test_split(doc_en, doc_fr, test_size=0.1, random_state=42)\n",
        "\n",
        "# --- create Language index, vocabulary etc --- #\n",
        "\n",
        "# source\n",
        "LanguageEN = LanguageIndex(source_train, threshold=0, target=False)\n",
        "doc_en = doc_en[0:10]\n",
        "_ = LanguageEN.ConvertTextToIndex(LanguageEN.lang, target=False)\n",
        "print(\"English first sentence in train source\")\n",
        "print(LanguageEN.lang[0])\n",
        "print(LanguageEN.ConvertIndexToSentence(LanguageEN.lang[0]))\n",
        "\n",
        "\n",
        "\n",
        "# target\n",
        "\n",
        "LanguageFR = LanguageIndex(target_train, threshold = 0, target=True)\n",
        "doc_fr = doc_fr[0:10]\n",
        "_ = LanguageFR.ConvertTextToIndex(LanguageFR.lang, target=True)\n",
        "print(\"French first sentence in train source\")\n",
        "print(LanguageFR.lang[0])\n",
        "print(LanguageFR.ConvertIndexToSentence(LanguageFR.lang[0]))\n",
        "list(LanguageFR.word2idx)[:10]\n",
        "\n",
        "\n",
        "\n",
        "source_int_text_train, target_int_text_train, source_vocab_to_int, target_vocab_to_int = LanguageEN.lang, LanguageFR.lang, LanguageEN.word2idx, LanguageFR.word2idx\n",
        "source_int_text_test = LanguageEN.ConvertTextToIndex(source_test, target=False, inplace=False)\n",
        "target_int_text_test = LanguageFR.ConvertTextToIndex(target_test, target=True)\n",
        "\n",
        "print(\"French : \\n Number of train : {0}  Vocabulary : {1} words \\nNumber of test : {2}\".format(len(target_train), len(LanguageFR.vocab), len(target_test) ))\n",
        "print(\"English : \\n Number of train : {0}  Vocabulary : {1} words \\nNumber of test : {2}\".format(len(source_train), len(LanguageEN.vocab), len(source_test) ))\n",
        "print(len(LanguageFR.word2idx), len(LanguageEN.word2idx))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## --- French --- ##\n",
            "number of sentences : 137860\n",
            "min: 4 -- max: 23\n",
            "## --- English --- ##\n",
            "number of sentences : 137860\n",
            "min: 4 -- max: 17\n",
            "English first sentence in train source\n",
            "[125, 95, 91, 141, 55, 94, 2, 31, 93, 91, 39, 89, 128, 3]\n",
            "['new', 'jersey', 'is', 'pleasant', 'during', 'january', ',', 'but', 'it', 'is', 'cold', 'in', 'november', '.']\n",
            "French first sentence in train source\n",
            "[200, 161, 113, 8, 104, 158, 4, 181, 152, 113, 133, 104, 208, 5, 3]\n",
            "['new', 'jersey', 'est', 'agreable', 'en', 'janvier', ',', 'mais', 'il', 'est', 'froid', 'en', 'novembre', '.', '<EOS>']\n",
            "French : \n",
            " Number of train : 124074  Vocabulary : 331 words \n",
            "Number of test : 13786\n",
            "English : \n",
            " Number of train : 124074  Vocabulary : 205 words \n",
            "Number of test : 13786\n",
            "331 205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mLYUIRdwp5ZM",
        "colab_type": "code",
        "outputId": "5cd56ae3-db9d-4d77-99f6-3e634a965a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "cell_type": "code",
      "source": [
        "import math, time\n",
        "\n",
        "save_path = 'checkpoints/dev'\n",
        "\n",
        "#max_target_sentence_length = max([len(sentence) for sentence in source_int_text_train])\n",
        "\n",
        "display_step = 300\n",
        "\n",
        "epochs = 13\n",
        "batch_size = 128\n",
        "\n",
        "rnn_size = 128\n",
        "num_layers = 3\n",
        "\n",
        "encoding_embedding_size = 200\n",
        "decoding_embedding_size = 200\n",
        "\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.5\n",
        "\n",
        "#source_test = [['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.'],\n",
        "#               ['it','is', 'snowy'], ['he', 'is', 'quiet', '.'], ['paris', 'is', 'very', 'quiet', '.']]\n",
        "#source_test = LanguageEN.ConvertTextToIndex(source_test, False, False)\n",
        "#print(source_test)\n",
        "#source_test = pad_sequences(source_test, padding='post', value=source_vocab_to_int['<PAD>'])\n",
        "#source_test_lengths = [ int(len(s)) for s in source_test]\n",
        "#target_test_lengths = [ int(len(s)*1.5) for s in source_test]\n",
        "## input_data, targets, target_sequence_length and max_target_sequence_length are variable\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    input_data, targets, source_sequence_length, target_sequence_length, max_target_sequence_length = init_placeholders()\n",
        "    lr, keep_prob = tf.placeholder(tf.float32, name='lr_rate'), tf.placeholder(tf.float32, name='keep_prob')\n",
        "    size_batch = tf.shape(input_data)[0]\n",
        "\n",
        "            \n",
        "            \n",
        "    train_logits, inference_logits = seq2seq_model(input_data = tf.reverse(input_data, [-1]),\n",
        "                                                   target_data = targets,\n",
        "                                                   keep_prob = keep_prob,\n",
        "                                                   batch_size = size_batch,\n",
        "                                                   source_sequence_length = source_sequence_length,\n",
        "                                                   target_sequence_length = target_sequence_length,\n",
        "                                                   max_target_sentence_length = max_target_sequence_length,\n",
        "                                                   source_vocab_size = len(source_vocab_to_int),\n",
        "                                                   target_vocab_size = len(target_vocab_to_int),\n",
        "                                                   enc_embedding_size = encoding_embedding_size,\n",
        "                                                   dec_embedding_size = decoding_embedding_size,\n",
        "                                                   rnn_size = rnn_size,\n",
        "                                                   num_layers = num_layers,\n",
        "                                                   target_vocab_to_int = target_vocab_to_int,\n",
        "                                                   use_lstm=False)\n",
        "    \n",
        "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
        "    # - Returns a mask tensor representing the first N positions of each cell.\n",
        "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function - weighted softmax cross entropy\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "        #train_op = optimizer.minimize(cost)\n",
        "       \n",
        "    a = tf.concat( [tf.fill([size_batch, 1], 2), targets], 1)\n",
        "   \n",
        "  \n",
        "# --- Training --- #  \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    epochs = 20\n",
        "    for e in range(epochs):\n",
        "      total_loss_train = 0\n",
        "      total_acc_train = 0\n",
        "      total_bleu_train = 0\n",
        "      nan_value = 0\n",
        "      nonan_value = 0\n",
        "      divider = 0\n",
        "      inf_value = 0\n",
        "      begin = time.time()\n",
        "    \n",
        "    \n",
        "      # --- Training Step --- #\n",
        "      for iteration, (source_batch, target_batch, source_lengths, target_lengths) in enumerate(get_batch(source_int_text_train, target_int_text_train, source_vocab_to_int['<PAD>'], target_vocab_to_int['<PAD>'], batch_size=batch_size, training=True)):\n",
        "\n",
        "          batch_train_logits, _, loss_train = sess.run(\n",
        "                  [inference_logits, train_op, cost],\n",
        "                  {input_data: source_batch,\n",
        "                   source_sequence_length: source_lengths,\n",
        "                   targets: target_batch,\n",
        "                   lr: learning_rate,\n",
        "                   target_sequence_length: target_lengths,\n",
        "                   keep_prob: keep_probability})\n",
        "\n",
        "          \n",
        "\n",
        "          if math.isnan(loss_train):\n",
        "            nan_value += 1\n",
        "            #print(True)\n",
        "            #print('nan value')\n",
        "          elif math.isinf(loss_train):\n",
        "            inf_value += 1\n",
        "            #print('inf value')\n",
        "          else:\n",
        "            nonan_value += 1\n",
        "            divider += len(source_batch)\n",
        "            total_loss_train = total_loss_train + loss_train * len(source_batch)\n",
        "            total_acc_train = total_acc_train + get_accuracy(target_batch, batch_train_logits)*len(source_batch)\n",
        "            total_bleu_train = total_bleu_train + np.mean(bleu_metric( LanguageFR.ConvertIndexToText(target_batch), LanguageFR.ConvertIndexToText(batch_train_logits)) )* len(source_batch)\n",
        "          #if iteration % display_step == 0 and iteration > 0:\n",
        "          #      batch_train_logits = sess.run(\n",
        "          #          inference_logits,\n",
        "          #          {input_data: source_batch,\n",
        "          #           source_sequence_length: source_lengths,\n",
        "          #           target_sequence_length: target_lengths,\n",
        "          #           keep_prob: 1.0})\n",
        "          #      train_acc = get_accuracy(target_batch, batch_train_logits)\n",
        "          #      print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, , Loss: {:>6.4f}'\n",
        "          #            .format(e, iteration, len(source_int_text) // batch_size, train_acc,  loss))  \n",
        "          #      print(target_batch[0])\n",
        "          #      print(batch_train_logits[0])\n",
        "\n",
        "               \n",
        "\n",
        "\n",
        "      if divider == 0:\n",
        "          divider = 1\n",
        "      print(\"Epoch n°{0} Loss Train : {1} accuracy : {2}  Bleu : {6}  real loss value iteration : {3}/{4}   Time : {5} sec \".format(e+1, total_loss_train/divider, total_acc_train/divider, \n",
        "                                                                                                                              nonan_value, nan_value + nonan_value + inf_value, time.time() - begin, total_bleu_train/divider))\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "    \n",
        "      # --- Testing Step --- #\n",
        "      total_loss_test = 0\n",
        "      total_acc_test = 0\n",
        "      total_bleu_test = 0\n",
        "      nan_value = 0\n",
        "      nonan_value = 0\n",
        "      divider = 0\n",
        "      inf_value = 0\n",
        "      begin = time.time()\n",
        "      for iteration, (source_batch, target_batch, source_lengths, target_lengths) in enumerate(get_batch(source_int_text_test, target_int_text_test, source_vocab_to_int['<PAD>'], target_vocab_to_int['<PAD>'], batch_size=batch_size, training=False)):\n",
        "\n",
        "          batch_test_logits, loss_test = sess.run(\n",
        "                  [inference_logits, cost],\n",
        "                  {input_data: source_batch,\n",
        "                   source_sequence_length: source_lengths,\n",
        "                   targets: target_batch,\n",
        "                   lr: learning_rate,\n",
        "                   target_sequence_length: target_lengths,\n",
        "                   keep_prob: keep_probability})\n",
        "\n",
        "          \n",
        "\n",
        "          if math.isnan(loss_train):\n",
        "            nan_value += 1\n",
        "            #print(True)\n",
        "            #print('nan value')\n",
        "          elif math.isinf(loss_train):\n",
        "            inf_value += 1\n",
        "            #print('inf value')\n",
        "          else:\n",
        "            nonan_value += 1\n",
        "            divider += len(source_batch)\n",
        "            total_loss_test = total_loss_test + loss_test * len(source_batch)\n",
        "            total_acc_test = total_acc_test + get_accuracy(target_batch, batch_test_logits)*len(source_batch)\n",
        "            total_bleu_test = total_bleu_test + np.mean(bleu_metric( LanguageFR.ConvertIndexToText(target_batch), LanguageFR.ConvertIndexToText(batch_test_logits)) )* len(source_batch)\n",
        "              \n",
        "\n",
        "\n",
        "      if divider == 0:\n",
        "          divider = 1\n",
        "      print(\"Epoch n°{0} Loss testing : {1} accuracy : {2}  Bleu : {6}  real loss value iteration : {3}/{4}   Time : {5} sec \".format(e+1, total_loss_test/divider, total_acc_test/divider, \n",
        "                                                                                                                              nonan_value, nan_value + nonan_value + inf_value, time.time() - begin, total_bleu_test/divider))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch n°1 Loss Train : 1.559560842250769 accuracy : 0.49457701597773107  Bleu : 0.3999663590839819  real loss value iteration : 969/969   Time : 202.70132899284363 sec \n",
            "Epoch n°1 Loss testing : 0.8095747919451355 accuracy : 0.5635854428746913  Bleu : 0.41494418112086  real loss value iteration : 460/460   Time : 42.832088470458984 sec \n",
            "Epoch n°2 Loss Train : 0.6607245754900363 accuracy : 0.634800016362623  Bleu : 0.457556112890772  real loss value iteration : 969/969   Time : 203.3954029083252 sec \n",
            "Epoch n°2 Loss testing : 0.49337017032900554 accuracy : 0.6810230928571407  Bleu : 0.5340005057220807  real loss value iteration : 460/460   Time : 44.01312756538391 sec \n",
            "Epoch n°3 Loss Train : 0.3459243895437941 accuracy : 0.7671368705307182  Bleu : 0.6604123219442335  real loss value iteration : 969/969   Time : 204.63078355789185 sec \n",
            "Epoch n°3 Loss testing : 0.2425968421810375 accuracy : 0.8127751597780309  Bleu : 0.7629831802369508  real loss value iteration : 460/460   Time : 44.24645733833313 sec \n",
            "Epoch n°4 Loss Train : 0.18097533172590444 accuracy : 0.8552261933427329  Bleu : 0.8223627875726973  real loss value iteration : 969/969   Time : 204.69526267051697 sec \n",
            "Epoch n°4 Loss testing : 0.14710803699285013 accuracy : 0.8659471966682778  Bleu : 0.8492909073744126  real loss value iteration : 460/460   Time : 43.966663122177124 sec \n",
            "Epoch n°5 Loss Train : 0.12093747839316249 accuracy : 0.8886747968072322  Bleu : 0.8742557288296796  real loss value iteration : 969/969   Time : 202.7184658050537 sec \n",
            "Epoch n°5 Loss testing : 0.10649837386017093 accuracy : 0.8935480580123887  Bleu : 0.8901373631852422  real loss value iteration : 460/460   Time : 43.76942181587219 sec \n",
            "Epoch n°6 Loss Train : 0.09267368582456965 accuracy : 0.9089532963825239  Bleu : 0.8994260337735106  real loss value iteration : 969/969   Time : 203.2560520172119 sec \n",
            "Epoch n°6 Loss testing : 0.0868105099100204 accuracy : 0.9100942067282825  Bleu : 0.906134678504082  real loss value iteration : 460/460   Time : 43.859824895858765 sec \n",
            "Epoch n°7 Loss Train : 0.07663832333465602 accuracy : 0.9209902817842275  Bleu : 0.9142538683556503  real loss value iteration : 969/969   Time : 202.67503762245178 sec \n",
            "Epoch n°7 Loss testing : 0.07291118771347964 accuracy : 0.9240501501149226  Bleu : 0.920299180688201  real loss value iteration : 460/460   Time : 43.95426821708679 sec \n",
            "Epoch n°8 Loss Train : 0.06618373240541256 accuracy : 0.9296752024736816  Bleu : 0.9241193501097542  real loss value iteration : 969/969   Time : 203.2548611164093 sec \n",
            "Epoch n°8 Loss testing : 0.0659555376542606 accuracy : 0.9290374996814414  Bleu : 0.9266013514701774  real loss value iteration : 460/460   Time : 43.574755907058716 sec \n",
            "Epoch n°9 Loss Train : 0.05747963306450807 accuracy : 0.9375512324080286  Bleu : 0.9332334043438095  real loss value iteration : 969/969   Time : 201.85645699501038 sec \n",
            "Epoch n°9 Loss testing : 0.057207501608103915 accuracy : 0.9365553865826846  Bleu : 0.9357121972630769  real loss value iteration : 460/460   Time : 43.911707162857056 sec \n",
            "Epoch n°10 Loss Train : 0.05255756192524428 accuracy : 0.9418285532444962  Bleu : 0.9384420451573606  real loss value iteration : 969/969   Time : 203.53130960464478 sec \n",
            "Epoch n°10 Loss testing : 0.05348718656900117 accuracy : 0.9393684525697742  Bleu : 0.9402886428928793  real loss value iteration : 460/460   Time : 44.030113220214844 sec \n",
            "Epoch n°11 Loss Train : 0.048157458098449575 accuracy : 0.9460020348842998  Bleu : 0.9436284825129851  real loss value iteration : 969/969   Time : 202.6457121372223 sec \n",
            "Epoch n°11 Loss testing : 0.050731790275133935 accuracy : 0.9435004407386754  Bleu : 0.943775817035443  real loss value iteration : 460/460   Time : 44.02138829231262 sec \n",
            "Epoch n°12 Loss Train : 0.04409616408433277 accuracy : 0.9496504712037711  Bleu : 0.9482221562342026  real loss value iteration : 969/969   Time : 203.50049471855164 sec \n",
            "Epoch n°12 Loss testing : 0.04889769446786807 accuracy : 0.9436044952381136  Bleu : 0.9466592502196478  real loss value iteration : 460/460   Time : 44.00732350349426 sec \n",
            "Epoch n°13 Loss Train : 0.03941675745550508 accuracy : 0.9539472743445291  Bleu : 0.9529129768666239  real loss value iteration : 969/969   Time : 201.9840190410614 sec \n",
            "Epoch n°13 Loss testing : 0.043060662025734664 accuracy : 0.9510680154334751  Bleu : 0.952077448625474  real loss value iteration : 460/460   Time : 43.609700441360474 sec \n",
            "Epoch n°14 Loss Train : 0.037150585232927275 accuracy : 0.9567852422043053  Bleu : 0.9556248356963702  real loss value iteration : 969/969   Time : 202.18652391433716 sec \n",
            "Epoch n°14 Loss testing : 0.04085356408317249 accuracy : 0.9516327505080396  Bleu : 0.9535272871102962  real loss value iteration : 460/460   Time : 43.95619606971741 sec \n",
            "Epoch n°15 Loss Train : 0.03512948048104012 accuracy : 0.9591532966592006  Bleu : 0.9584140085693045  real loss value iteration : 969/969   Time : 203.3718671798706 sec \n",
            "Epoch n°15 Loss testing : 0.040774771319271264 accuracy : 0.9554226805820715  Bleu : 0.9553336546571919  real loss value iteration : 460/460   Time : 44.03491163253784 sec \n",
            "Epoch n°16 Loss Train : 0.03263443025161577 accuracy : 0.9602869713135491  Bleu : 0.9600219215073523  real loss value iteration : 969/969   Time : 204.22573161125183 sec \n",
            "Epoch n°16 Loss testing : 0.036312789123275124 accuracy : 0.9581148458787482  Bleu : 0.9594968207133943  real loss value iteration : 460/460   Time : 44.150824546813965 sec \n",
            "Epoch n°17 Loss Train : 0.030952525073306786 accuracy : 0.9620654919020293  Bleu : 0.9621597371958226  real loss value iteration : 969/969   Time : 204.56855249404907 sec \n",
            "Epoch n°17 Loss testing : 0.036281836015755575 accuracy : 0.9564867364503921  Bleu : 0.9600928532472768  real loss value iteration : 460/460   Time : 44.09421157836914 sec \n",
            "Epoch n°18 Loss Train : 0.02943250946258459 accuracy : 0.9638852780963184  Bleu : 0.9634979369095423  real loss value iteration : 969/969   Time : 203.3267900943756 sec \n",
            "Epoch n°18 Loss testing : 0.03300900979996556 accuracy : 0.9591578484140701  Bleu : 0.9623106327387463  real loss value iteration : 460/460   Time : 44.08460474014282 sec \n",
            "Epoch n°19 Loss Train : 0.028104115601425263 accuracy : 0.9653838393456093  Bleu : 0.9652038265761342  real loss value iteration : 969/969   Time : 203.39824438095093 sec \n",
            "Epoch n°19 Loss testing : 0.03364470000234894 accuracy : 0.9593322620369515  Bleu : 0.9624229135547379  real loss value iteration : 460/460   Time : 44.13898062705994 sec \n",
            "Epoch n°20 Loss Train : 0.026563299486319407 accuracy : 0.9670060926055281  Bleu : 0.9669330924556654  real loss value iteration : 969/969   Time : 203.9659707546234 sec \n",
            "Epoch n°20 Loss testing : 0.03320901263391727 accuracy : 0.9604769571123681  Bleu : 0.9638963291454962  real loss value iteration : 460/460   Time : 44.09199523925781 sec \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}